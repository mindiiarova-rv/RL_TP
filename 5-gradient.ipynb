{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "# 5. Gradient Methods\n",
    "\n",
    "This notebook presents gradient methods, useful for learning in some environment with a large state space.\n",
    "\n",
    "We use a neural network with a single hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import TicTacToe, Nim, ConnectFour\n",
    "from agent import Agent, OnlineEvaluation\n",
    "from dynamic import ValueIteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with value-based methods. The neural network is a regressor that approximates the value function. Note that the model is supposed to be known, so that we don't need the action-value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regressor(nn.Module):\n",
    "    \"\"\"Neural network for value function approximation. Return the value of each state.\"\"\"\n",
    "    def __init__(self, model, hidden_size):\n",
    "        if not hasattr(model, 'one_hot_encode'):\n",
    "            raise ValueError(\"The environment must have a one-hot encoding of states.\")   \n",
    "        super(Regressor, self).__init__()\n",
    "        self.model = model\n",
    "        state = model.init_state()\n",
    "        code = model.one_hot_encode(state)\n",
    "        input_size = len(code)\n",
    "        self.nn = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size), \n",
    "            nn.GELU(), \n",
    "            nn.Linear(hidden_size, 1))\n",
    "\n",
    "    def forward(self, code):\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        return self.nn(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = TicTacToe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = Regressor(model=game, hidden_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,\n",
       " array([[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = game.state\n",
    "# one-hot encoding\n",
    "code = game.one_hot_encode(state)\n",
    "# tensor\n",
    "code = torch.tensor(code).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "value = regressor.forward(code).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0137])\n"
     ]
    }
   ],
   "source": [
    "# value of the state\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To do\n",
    "\n",
    "* Complete the method get_best_actions of the class ValueGradient. \n",
    "* Test the agent on TicTacToe, against (1) a random adversary and (2) a perfect adversary.\n",
    "* Test the agent on ConnectFour, against (1) a random adversary and (2) an adversary with the one-step policy.\n",
    "* Compare your results to another learning strategy (e.g., Monte-Carlo learning) and interpret the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueGradient(OnlineEvaluation):\n",
    "    \"\"\"Agent learning by value gradient. The model is supposed to be known.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : object of class Environment\n",
    "        Model.\n",
    "    player : int\n",
    "        Player for games (1 or -1, default = default player of the game).\n",
    "    gamma : float\n",
    "        Discount rate (in [0, 1], default = 1).\n",
    "    hidden_size : int\n",
    "        Size of the hidden layer (default = 100).\n",
    "    \"\"\"\n",
    "    def __init__(self, model, player=None, gamma=1, hidden_size=100):\n",
    "        super(ValueGradient, self).__init__(model, player=player)  \n",
    "        if not hasattr(model, \"get_next_state\"):\n",
    "            raise ValueError(\"The model must be known, with a 'get_next_state' method.\")\n",
    "        self.nn = Regressor(model, hidden_size)\n",
    "        self.gamma = gamma\n",
    "        self.model = model\n",
    "    \n",
    "    def get_best_actions(self, state):\n",
    "        \"\"\"Get the best actions in some state according to the value function.\"\"\"\n",
    "        actions = self.get_actions(state)\n",
    "        if len(actions) > 1:\n",
    "            values = []\n",
    "\n",
    "            code = self.model.one_hot_encode(state)\n",
    "            code = torch.tensor(code).float()\n",
    "            current_value = self.nn.forward(code).detach()\n",
    "\n",
    "            for act in actions:\n",
    "                player, next_state = self.model.get_next_state(state, act)\n",
    "                next_state = (player, next_state)\n",
    "                \n",
    "                if self.model.is_terminal(next_state):\n",
    "                    values.append(current_value)\n",
    "                else:\n",
    "                    next_code = self.model.one_hot_encode(next_state)\n",
    "                    next_code = torch.tensor(next_code).float()\n",
    "                    next_value = self.nn.forward(next_code).detach()\n",
    "                    values.append(next_value)\n",
    "\n",
    "            if self.player == 1:\n",
    "                best_value = max(values)\n",
    "            else:\n",
    "                best_value = min(values)\n",
    "            actions = [action for action, value in zip(actions, values) if value == best_value]\n",
    "\n",
    "        return actions      \n",
    "    \n",
    "    def update_policy(self):\n",
    "        self.policy = self.get_policy()\n",
    "    \n",
    "    def get_samples(self, horizon, epsilon):\n",
    "        \"\"\"Get samples from one episode under the epsilon-greedy policy.\"\"\"\n",
    "        self.policy = self.randomize_policy(epsilon=epsilon)\n",
    "        self.model.reset()\n",
    "        state = self.model.state\n",
    "        states = []\n",
    "        rewards = []\n",
    "        for t in range(horizon):\n",
    "            action = self.get_action(state)\n",
    "            reward, stop = self.model.step(action)\n",
    "            states.append(state)\n",
    "            rewards.append(reward)\n",
    "            if stop:\n",
    "                break\n",
    "            state = self.model.state\n",
    "        gains = []\n",
    "        gain = 0\n",
    "        for reward in reversed(rewards):\n",
    "            gain = reward + self.gamma * gain\n",
    "            gains.append(gain)\n",
    "        return reversed(states), gains\n",
    "        \n",
    "    def train(self, horizon=100, n_episodes=1000, learning_rate=0.01, epsilon=0.1):\n",
    "        \"\"\"Train the neural network with samples drawn from the epsilon-greedy policy.\"\"\"\n",
    "        optimizer = optim.Adam(self.nn.parameters(), lr=learning_rate)\n",
    "        for t in range(n_episodes):\n",
    "            states, gains = self.get_samples(horizon, epsilon)\n",
    "            codes = [self.model.one_hot_encode(state) for state in states]\n",
    "            codes = np.array(codes)\n",
    "            codes = torch.tensor(codes).float()\n",
    "            values = self.nn.forward(codes)\n",
    "            gains = torch.tensor(gains).float().reshape(-1, 1)\n",
    "            mse = nn.MSELoss()\n",
    "            loss = mse(values, gains)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "Game = TicTacToe\n",
    "game = TicTacToe()\n",
    "agent = ValueGradient(game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.277"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gains = agent.get_gains(n_runs=1000)\n",
    "np.mean(gains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.train(n_episodes=1000)\n",
    "agent.update_policy()\n",
    "gains = agent.get_gains(n_runs=1000)\n",
    "np.mean(gains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Game against perfect adversary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = ValueIteration(game)\n",
    "policy, ad_policy = algo.get_perfect_players()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game = Game(adversary_policy=ad_policy)\n",
    "agent = ValueGradient(game)\n",
    "\n",
    "agent.train(n_episodes=100)\n",
    "agent.update_policy()\n",
    "gains = agent.get_gains(n_runs=100)\n",
    "np.mean(gains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results, we see that the method works well against the random policy of the opponent, but against the perfect policy it shows poor results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ConnectFour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "Game = ConnectFour\n",
    "game = Game(adversary_policy=\"random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ValueGradient(game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.train(n_episodes=100)\n",
    "agent.update_policy()\n",
    "gains = agent.get_gains(n_runs=100)\n",
    "np.mean(gains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.76"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game = Game(adversary_policy=\"one_step\")\n",
    "agent = ValueGradient(game)\n",
    "agent.train(n_episodes=100)\n",
    "agent.update_policy()\n",
    "gains = agent.get_gains(n_runs=100)\n",
    "np.mean(gains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "similar to tic-tac-toe the results on the 1 step policy are bad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monte-Carlo Tic-Tac-Toe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCLearning(OnlineEvaluation):\n",
    "    \"\"\"Online evaluation by Monte-Carlo.\"\"\"\n",
    "        \n",
    "    def update_values(self, state=None, horizon=100):\n",
    "        \"\"\"Update the values from one episode.\"\"\"\n",
    "        stop, states, rewards = self.get_episode(state=state, horizon=horizon)\n",
    "        gain = 0\n",
    "        # backward update\n",
    "        states.pop()\n",
    "        for state, reward in zip(reversed(states), reversed(rewards)):\n",
    "            self.add_state(state)\n",
    "            code = self.model.encode(state)\n",
    "            self.count[code] += 1\n",
    "            # to be modified\n",
    "            # begin\n",
    "            gain = self.gamma * gain + reward\n",
    "            # end \n",
    "            diff = gain - self.value[code]\n",
    "            count = self.count[code]\n",
    "            self.value[code] += diff / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "Game = TicTacToe\n",
    "game = Game(adversary_policy=\"random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = MCLearning(game, policy='random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 100\n",
    "values_mc = []\n",
    "for t in range(n_episodes):\n",
    "    algo.update_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = algo.get_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(game, policy=policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(agent.get_gains())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "Game = TicTacToe\n",
    "game = Game(adversary_policy=\"one_step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = MCLearning(game)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 1000\n",
    "values_mc = []\n",
    "for t in range(n_episodes):\n",
    "    algo.update_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = algo.get_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(game, policy=policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(agent.get_gains())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike valuegradient, ms learning shows good results in both cases.\n",
    "However, on a random policy the results of the gradient are slightly better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now consider a policy-based method. The neural network is a classifier that approximates the optimal policy. It returns the probability of each action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    \"\"\"Neural network for policy gradient. Return the distribution of actions in each state.\"\"\"\n",
    "    def __init__(self, model, hidden_size):\n",
    "        if not hasattr(model, 'one_hot_encode'):\n",
    "            raise ValueError(\"The environment must have a one-hot encoding of states.\")   \n",
    "        super(Classifier, self).__init__()\n",
    "        self.model = model\n",
    "        actions = model.get_all_actions()\n",
    "        if self.model.is_game():\n",
    "            # remove action when passing\n",
    "            actions.pop()\n",
    "        self.actions = actions\n",
    "        state = model.init_state()\n",
    "        code = model.one_hot_encode(state)\n",
    "        input_size = len(code)\n",
    "        output_size = len(actions)\n",
    "        self.nn = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size), \n",
    "            nn.GELU(), \n",
    "            nn.Linear(hidden_size, output_size), \n",
    "            nn.Softmax(dim=0))\n",
    "\n",
    "    def forward(self, code):\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        return self.nn(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = TicTacToe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Classifier(model=game, hidden_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = game.state\n",
    "# one-hot encoding\n",
    "code = game.one_hot_encode(state)\n",
    "# tensor\n",
    "code = torch.tensor(code).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = classifier.forward(code).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1132, 0.1117, 0.1055, 0.1150, 0.1233, 0.1130, 0.1044, 0.1002, 0.1137])\n"
     ]
    }
   ],
   "source": [
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To do\n",
    "\n",
    "* Complete the method 'train' of the class PolicyGradient. Observe that a penalty is assigned for illegal actions.\n",
    "* Test the agent on TicTacToe, against (1) a random adversary and (2) a perfect adversary.\n",
    "* Test the agent on ConnectFour, against (1) a random adversary and (2) an adversary with the one-step policy.\n",
    "* Compare your results to another learning strategy (e.g., Monte-Carlo learning) and interpret the results.\n",
    "* (bonus) Try to improve policy gradient on TicTacToe with a perfect adversary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyGradient(Agent):\n",
    "    \"\"\"Agent learning by policy gradient.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : object of class Environment\n",
    "        Model.a\n",
    "    player : int\n",
    "        Player for games (1 or -1, default = default player of the game).\n",
    "    gamma : float\n",
    "        Discount rate (in [0, 1], default = 1).\n",
    "    hidden_size : int\n",
    "        Size of the hidden layer (default = 100).\n",
    "    penalty : float\n",
    "        Penalty for illegal actions (default = -5).\n",
    "    min_log : float\n",
    "        Minimal value to compute the log (default = 1e-10)\n",
    "    \"\"\"\n",
    "    def __init__(self, model, player=None, gamma=1, hidden_size=100, penalty=-1, min_log=1e-10):\n",
    "        super(PolicyGradient, self).__init__(model, player=player)  \n",
    "        self.nn = Classifier(model, hidden_size)\n",
    "        self.action_id = {action: i for i, action in enumerate(self.nn.actions)}\n",
    "        self.gamma = gamma\n",
    "        self.penalty = penalty\n",
    "        self.min_log = min_log\n",
    "        \n",
    "    def get_policy(self):\n",
    "        \"\"\"Get the current policy.\"\"\"\n",
    "        def policy(state):\n",
    "            actions = self.model.get_actions(state)\n",
    "            if len(actions) > 1:\n",
    "                win_actions = []\n",
    "                # check win actions\n",
    "                if self.model.is_game():\n",
    "                    next_states = [self.model.get_next_state(state, action) for action in actions]\n",
    "                    win_actions = [self.model.get_reward(next_state) == self.player for next_state in next_states]\n",
    "                if any(win_actions):\n",
    "                    probs = np.array(win_actions).astype(float)\n",
    "                else:\n",
    "                    # get prob of each action\n",
    "                    code = self.model.one_hot_encode(state)\n",
    "                    code = torch.tensor(code).float()\n",
    "                    probs = self.nn.forward(code)\n",
    "                    probs = probs.detach().numpy()\n",
    "                    # restrict to available actions\n",
    "                    indices = [self.action_id[action] for action in actions]\n",
    "                    probs = probs[indices]                    \n",
    "                # renormalize\n",
    "                if np.sum(probs) > 0:\n",
    "                    probs = probs / np.sum(probs)\n",
    "                else:\n",
    "                    probs = np.ones(len(actions)) / len(actions)\n",
    "            else:\n",
    "                probs = [1]\n",
    "            return probs, actions\n",
    "        return policy\n",
    "    \n",
    "    def update_policy(self):\n",
    "        \"\"\"Update the policy.\"\"\"\n",
    "        self.policy = self.get_policy()\n",
    "    \n",
    "    def get_samples(self, horizon):\n",
    "        \"\"\"Get samples from one episode.\"\"\"\n",
    "        self.update_policy()\n",
    "        rewards = []\n",
    "        log_probs = []\n",
    "        log_probs_illegal = []\n",
    "        self.model.reset()\n",
    "        state = self.model.state\n",
    "        for t in range(horizon):\n",
    "            action = self.get_action(state)\n",
    "            if action is not None:\n",
    "                i = self.action_id[action]\n",
    "                code = self.model.one_hot_encode(state)\n",
    "                code = torch.tensor(code).float()\n",
    "                probs = self.nn.forward(code)\n",
    "                prob = torch.clip(probs[i], self.min_log, 1 - self.min_log)\n",
    "                log_prob = torch.log(prob).reshape(1)\n",
    "\n",
    "                actions = self.model.get_actions(state)\n",
    "                if action in actions:\n",
    "                    reward, stop = self.model.step(action)\n",
    "                    state = self.model.state\n",
    "                    rewards.append(reward)\n",
    "                    log_probs.append(log_prob)\n",
    "                else:\n",
    "                    log_probs_illegal.append(log_prob)\n",
    "            else:\n",
    "                reward, stop = self.model.step(action)\n",
    "                state = self.model.state\n",
    "            if stop:\n",
    "                break\n",
    "        gain = 0\n",
    "        for reward in reversed(rewards):\n",
    "            gain = reward + self.gamma * gain\n",
    "        return gain, log_probs, log_probs_illegal\n",
    "        \n",
    "    def train(self, horizon=100, n_episodes=1000, learning_rate=0.01):\n",
    "        \"\"\"Train the neural network.\"\"\"\n",
    "        optimizer = optim.Adam(self.nn.parameters(), lr=learning_rate)\n",
    "        for t in range(n_episodes):\n",
    "            gain, log_probs, log_probs_illegal = self.get_samples(horizon)\n",
    "            loss = 0\n",
    "            if len(log_probs):\n",
    "                log_probs = torch.cat(log_probs)\n",
    "                loss -= gain * torch.sum(log_probs)\n",
    "            if len(log_probs_illegal):\n",
    "                log_probs_illegal = torch.cat(log_probs_illegal)\n",
    "                loss += self.penalty * torch.sum(log_probs_illegal)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = TicTacToe()\n",
    "agent = PolicyGradient(game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.train(n_episodes=100)\n",
    "agent.update_policy()\n",
    "gains = agent.get_gains(n_runs=100)\n",
    "np.mean(gains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Game with perfect adversary policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = ValueIteration(game)\n",
    "policy, ad_policy = algo.get_perfect_players()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "Game = TicTacToe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.74"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game = Game(adversary_policy=ad_policy)\n",
    "agent = PolicyGradient(game)\n",
    "\n",
    "agent.train(n_episodes=100)\n",
    "agent.update_policy()\n",
    "gains = agent.get_gains(n_runs=100)\n",
    "np.mean(gains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ConnectFour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "Game = ConnectFour\n",
    "game = Game(adversary_policy=\"random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = PolicyGradient(game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.train(n_episodes=100)\n",
    "agent.update_policy()\n",
    "gains = agent.get_gains(n_runs=100)\n",
    "np.mean(gains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a game with a more complex space (connect four) the policy gradient shows results significantly better than the value gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-step policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = Game(adversary_policy=\"one_step\")\n",
    "agent = PolicyGradient(game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.54"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.train(n_episodes=100)\n",
    "agent.update_policy()\n",
    "gains = agent.get_gains(n_runs=100)\n",
    "np.mean(gains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "however, against a more complex policy the results are the same as in the previous method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In games like Tic Tac Toe and Connect Four, rewards are sparse (they occur only at the end of the game). This sparsity can make it difficult for policy gradient methods to effectively learn from the rewards."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
