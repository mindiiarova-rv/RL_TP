{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "# 4. Online control\n",
    "\n",
    "This notebook presents the **online control** of an agent by SARSA and Q-learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import TicTacToe, Nim, ConnectFour\n",
    "from agent import Agent, OnlineControl\n",
    "from dynamic import ValueIteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To do\n",
    "\n",
    "* Complete the class ``SARSA`` and test it on Tic-Tac-Toe.\n",
    "* Complete the class ``QLearning`` and test it on Tic-Tac-Toe.\n",
    "* Compare these algorithms on Tic-Tac-Toe (play first) and Nim (play second), using a random adversary, then a perfect adversary. Comment your results.\n",
    "* Test these algorithms on Connect 4 against a random adversary. Comment your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSA(OnlineControl):\n",
    "    \"\"\"Online control by SARSA.\"\"\"\n",
    "        \n",
    "    def update_values(self, state=None, horizon=100, epsilon=0.5):\n",
    "        \"\"\"Learn the action-value function online.\"\"\"\n",
    "        self.model.reset(state)\n",
    "        state = self.model.state\n",
    "        if not self.model.is_terminal(state):\n",
    "            action = self.randomize_best_action(state, epsilon=epsilon)\n",
    "            for t in range(horizon):\n",
    "                code = self.model.encode(state)\n",
    "                self.action_count[code][action] += 1\n",
    "                reward, stop = self.model.step(action)\n",
    "                # to be modified (get sample gain)\n",
    "                next_state = self.model.state\n",
    "                code_next = self.model.encode(next_state)\n",
    "                # begin\n",
    "                \n",
    "                if self.model.is_terminal(next_state):\n",
    "                    gain = reward  # No future rewards after a terminal state\n",
    "                else:\n",
    "                    next_action = self.randomize_best_action(next_state, epsilon=epsilon)\n",
    "                  \n",
    "                    gain = reward + self.gamma * self.action_value[code_next][next_action]\n",
    "                # end\n",
    "                diff = gain - self.action_value[code][action]\n",
    "                count = self.action_count[code][action]\n",
    "                self.action_value[code][action] += diff / count\n",
    "                if stop:\n",
    "                    break\n",
    "                # to be modified (update state and action)\n",
    "                # begin\n",
    "                state = next_state\n",
    "                action = next_action\n",
    "                # end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearning(OnlineControl):\n",
    "    \"\"\"Online control by Q-learning.\"\"\"\n",
    "        \n",
    "    def update_values(self, state=None, horizon=100, epsilon=0.5):\n",
    "        \"\"\"Learn the action-value function online.\"\"\"\n",
    "        self.model.reset(state)\n",
    "        state = self.model.state\n",
    "        all_actions = self.model.get_all_actions()\n",
    "        # to be completed\n",
    "        if not self.model.is_terminal(state):\n",
    "            for t in range(horizon):\n",
    "                action = self.randomize_best_action(state, epsilon=epsilon)\n",
    "                code = self.model.encode(state)\n",
    "\n",
    "                self.action_count[code][action] += 1\n",
    "                reward, stop = self.model.step(action)\n",
    "\n",
    "                next_state = self.model.state\n",
    "                code_next = self.model.encode(next_state)\n",
    "\n",
    "                if self.model.is_terminal(next_state):\n",
    "                    gain = reward\n",
    "                else:\n",
    "                   # print(self.action_value, \"STOP\", state, \"STOP\", code_next)\n",
    "                   # q_values = np.array([self.action_value[code_next][act] for act in all_actions])\n",
    "                    #max_act = np.argmax(q_values)\n",
    "                    #gain = reward + self.gamma * self.action_value[code_next][max_act]\n",
    "                    gain = reward + self.gamma * max(self.action_value[code_next].values(), default=0)\n",
    "                    \n",
    "                diff = gain - self.action_value[code][action]\n",
    "                count = self.action_count[code][action]\n",
    "                self.action_value[code][action] += diff / count\n",
    "                if stop:\n",
    "                    break\n",
    "                # to be modified (update state and action)\n",
    "                # begin\n",
    "                state = next_state\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "Game = TicTacToe\n",
    "game = Game(adversary_policy=\"random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23"
      ]
     },
     "execution_count": 519,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = Agent(game)\n",
    "np.mean(agent.get_gains())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 520,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Control = SARSA\n",
    "algo = Control(game)\n",
    "n_games = 1000\n",
    "\n",
    "for i in range(n_games):\n",
    "    algo.update_values(epsilon=0.1)\n",
    "policy = algo.get_policy()\n",
    "\n",
    "agent = Agent(game, policy)\n",
    "np.mean(agent.get_gains())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try perfect adversary policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.5"
      ]
     },
     "execution_count": 521,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo = ValueIteration(game)\n",
    "policy, ad_policy = algo.get_perfect_players()\n",
    "\n",
    "game = Game(adversary_policy=ad_policy)\n",
    "agent = Agent(game)\n",
    "algo = Control(game)\n",
    "\n",
    "n_games = 1000\n",
    "for i in range(n_games):\n",
    "    algo.update_values(epsilon=0.1)\n",
    "policy = algo.get_policy()\n",
    "agent = Agent(game, policy)\n",
    "\n",
    "np.mean(agent.get_gains())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "Game = Nim\n",
    "game = Game()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04"
      ]
     },
     "execution_count": 532,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = Agent(game)\n",
    "np.mean(agent.get_gains())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72"
      ]
     },
     "execution_count": 533,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Control = SARSA\n",
    "algo = Control(game)\n",
    "n_games = 1000\n",
    "\n",
    "for i in range(n_games):\n",
    "    algo.update_values(epsilon=0.1)\n",
    "policy = algo.get_policy()\n",
    "\n",
    "agent = Agent(game, policy)\n",
    "np.mean(agent.get_gains())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 534,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo = ValueIteration(game)\n",
    "policy, ad_policy = algo.get_perfect_players()\n",
    "\n",
    "game = Game(adversary_policy=ad_policy)\n",
    "agent = Agent(game)\n",
    "algo = Control(game)\n",
    "\n",
    "n_games = 1000\n",
    "for i in range(n_games):\n",
    "    algo.update_values(epsilon=0.1)\n",
    "policy = algo.get_policy()\n",
    "agent = Agent(game, policy)\n",
    "\n",
    "np.mean(agent.get_gains())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST Q Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "Game = TicTacToe\n",
    "game = Game(adversary_policy=\"random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27"
      ]
     },
     "execution_count": 529,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = Agent(game)\n",
    "np.mean(agent.get_gains())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86"
      ]
     },
     "execution_count": 530,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Control = QLearning\n",
    "algo = Control(game)\n",
    "\n",
    "n_games = 1000\n",
    "for i in range(n_games):\n",
    "    algo.update_values(epsilon=0.1)\n",
    "policy = algo.get_policy()\n",
    "agent = Agent(game, policy)\n",
    "\n",
    "np.mean(agent.get_gains())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.07"
      ]
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo = ValueIteration(game)\n",
    "policy, ad_policy = algo.get_perfect_players()\n",
    "\n",
    "game = Game(adversary_policy=ad_policy)\n",
    "agent = Agent(game)\n",
    "\n",
    "Control = QLearning\n",
    "algo = Control(game)\n",
    "\n",
    "n_games = 5000\n",
    "for i in range(n_games):\n",
    "    algo.update_values(epsilon=0.1)\n",
    "policy = algo.get_policy()\n",
    "agent = Agent(game, policy)\n",
    "\n",
    "np.mean(agent.get_gains())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "Game = Nim\n",
    "game = Game()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.06"
      ]
     },
     "execution_count": 536,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = Agent(game)\n",
    "np.mean(agent.get_gains())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76"
      ]
     },
     "execution_count": 537,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Control = QLearning\n",
    "algo = Control(game)\n",
    "\n",
    "n_games = 1000\n",
    "for i in range(n_games):\n",
    "    algo.update_values(epsilon=0.1)\n",
    "policy = algo.get_policy()\n",
    "agent = Agent(game, policy)\n",
    "\n",
    "np.mean(agent.get_gains())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 538,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo = ValueIteration(game)\n",
    "policy, ad_policy = algo.get_perfect_players()\n",
    "\n",
    "game = Game(adversary_policy=ad_policy)\n",
    "agent = Agent(game)\n",
    "\n",
    "Control = QLearning\n",
    "algo = Control(game)\n",
    "\n",
    "n_games = 5000\n",
    "for i in range(n_games):\n",
    "    algo.update_values(epsilon=0.1)\n",
    "policy = algo.get_policy()\n",
    "agent = Agent(game, policy)\n",
    "\n",
    "np.mean(agent.get_gains())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In games with a random policy (such as Tic-Tac-Toe and Nim), both SARSA and Q-learning algorithms can consistently achieve high gains because the opponent frequently makes suboptimal moves, allowing the algorithms to easily exploit these mistakes. However, against an ideal opponent who always makes optimal moves, both algorithms struggle to adapt and often lose, as the opponent's strategy is too strong and leaves little room for error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "Game = ConnectFour\n",
    "game = Game(adversary_policy=\"random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08"
      ]
     },
     "execution_count": 540,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = Agent(game)\n",
    "np.mean(agent.get_gains())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48"
      ]
     },
     "execution_count": 541,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Control = QLearning\n",
    "algo = Control(game)\n",
    "\n",
    "n_games = 10000\n",
    "for i in range(n_games):\n",
    "    algo.update_values(epsilon=0.1)\n",
    "policy = algo.get_policy()\n",
    "agent = Agent(game, policy)\n",
    "\n",
    "np.mean(agent.get_gains())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24"
      ]
     },
     "execution_count": 542,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Control = SARSA\n",
    "algo = Control(game)\n",
    "n_games = 10000\n",
    "\n",
    "for i in range(n_games):\n",
    "    algo.update_values(epsilon=0.1)\n",
    "policy = algo.get_policy()\n",
    "\n",
    "agent = Agent(game, policy)\n",
    "np.mean(agent.get_gains())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect Four requires a large number of episodes to achieve a reasonable gain because the game has a vast state and action space, making it challenging for algorithms like SARSA and Q-learning to explore and learn optimal strategies effectively. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
